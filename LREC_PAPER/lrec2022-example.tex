% LREC 2022 KC Example; 
% LREC Is now using templates similar to the ACL ones. 
\documentclass[10pt, a4paper]{article}
\usepackage{lrec2022} % this is the new LREC2022 Style
\usepackage{multibib}
\newcites{languageresource}{Language Resources}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{soul}
% for eps graphics
%%% References and Labels
%%% Reference labels without a punctuation 
% courtesy of Marc Schulder , uni Hamburg ****************
\usepackage{titlesec}
%\titleformat{\section}{\normalfont\large\bf\center}{\thesection.}{1em}{}
\titleformat{\section}{\normalfont\large\bfseries\center}{\thesection.}{1em}{}
\titleformat{\subsection}{\normalfont\SmallTitleFont\bfseries\raggedright}{\thesubsection.}{1em}{}
\titleformat{\subsubsection}{\normalfont\normalsize\bfseries\raggedright}{\thesubsubsection.}{1em}{}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}
%  ed 

\usepackage{epstopdf}
\usepackage[utf8]{inputenc}

\usepackage{hyperref}
\usepackage{xstring}

\usepackage{color}

\newcommand{\secref}[1]{\StrSubstitute{\getrefnumber{#1}}{.}{ }}

\title{\vspace*{.5\baselineskip} \textbf{Lexical Semantic Change Detection in Resource-Light, Morphologically Rich Languages: A Case Study in Emergent Modern Hebrew}}

\name{Raz Besaleli, Zachary Schultz} 

\address{Montclair State University \\
        1 Normal Ave \\
        Montclair, NJ 07043, USA \\
         % Address1, Address2, Address3 \\
         % author1@xxx.yy, author2@zzz.edu, author3@hhh.com\\
         \{besalelir1,schultzz1\}@montclair.edu\\}



\abstract{
In the past few years, lexical semantic change detection (LSCD) using contextualized word embeddings has been a topic that has gained a lot of interest in the NLP community. In this paper, we discuss the challenges of applying LSCD with contextualized word embeddings to Modern Hebrew, whose complex history, resource-light nature, rich morphology, and ambiguous morphology impose several significant experimental limitations.
 \\ \newline \Keywords{Emergent Modern Hebrew, Lexical Semantic Change Detection, NLP} }

\begin{document}

\maketitleabstract

\section{Introduction}

\subsection{On Emergent Modern Hebrew}
Modern Hebrew (MH) is a morphologically rich, resource-light language belonging to the Semitic branch of the Afro-Asiatic family. Its grammar is fusional and synthetic, with a complex verbal template system that often leads to significant morphological ambiguity, which is often also contributed to by its equally ambiguous orthography.
These traits make MH a particularly challenging language to work with in NLP. They, along with "language-agnostic" tools' general incompatibility with MH, have contributed to a significant deficiency of NLP resources for the language \cite{tsarfaty2019whats}.
Emergent Modern Hebrew, a particular chronolect of Modern Hebrew, is a historical period in which Hebrew was exposed to a significant amount of language contact due to significant political events such as war and mass immigration. It can be defined as beginning around 1881, marking the birth of the first native MH speaker, to around 1949, marking the establishment of the State of Israel. Although the nascence of Modern Hebrew in the early-mid 19th century has been widely studied, Emergent Hebrew has suffered a lack of research, mostly due to a lack of resources \cite{Rubinstein2019}.

\subsection{The Jerusalem Corpus of Emergent Modern Hebrew}
The Jerusalem Corpus of Emergent Modern Hebrew, is an annotated, multigenre corpus of Modern Hebrew texts written between the early 1720s to the late 1970s. It is composed of several subcorpora \cite{Rubinstein2019}:
\begin{itemize}
    \item \textbf{Street Ads:} A corpus of ephemera (street ads, posters, etc.) from 1862-1941.
    \item \textbf{Municipal Posters:} A corpus of ephemera from 1920-1960.
    \item \textbf{Dated BYP:} A corpus of literature, poetry, opinions, and journalism, taken from the Ben-Yehuda Project, from 1830-1970.
\end{itemize}

In this paper, we focus on an analysis of the Dated BYP subcorpus, in the time range 1881-1949, and omit the other subcorpora.
\subsection{Code}
The following repositories contain the code we used for this experiment:
\begin{itemize}
    \item The proprietary data frame for word usage matrices and corpus, code for distance measures, and code for extracting BERT embeddings can be found at \href{https://github.com/rbesaleli/dcwetk}{https://github.com/rbesaleli/dcwetk}.
    \item The code for the experiments can be found at *insert link here*.
\end{itemize}
\section{Related Work}
Applying unsupervised machine learning methods to contextualized word embeddings has proven to be an effective technique for diachronic LSCD, and has been shown to positively correlate with human judgments regarding diachronic semantic shift \cite{Giulianelli2020,Kutuzov2020}, especially when the contextualized word embeddings are extracted from ELMo \cite{Kutuzov2020}. The specific methods used are diverse, ranging from tracking the movement of word embedding clouds through semantic space, to measuring change in word usage distributions over time via word-sense induction using clustering.

There are several clustering methods that have been used for the latter method mentioned above. Clustering methods that require a user-defined number of clusters, such as K-Means Clustering  \cite{Giulianelli2020} or Agglomerative Clustering \cite{Laicher2021}, may be paired with the Silhouette Method to find the optimal number of clusters. Alternatively, a clustering method that predetermines the number of clusters may be used, such as Affinity Propagation \cite{Martinc2020,Kutuzov2020}. This is discussed further in \ref{clustering}.

That said, the performance of type-based embeddings has been shown to exceed the performance of token-based embeddings in LSCD, although the performance of the latter can be significantly improved by reducing word-form bias through methods such as lemmatization \cite{Laicher2021}, and in certain cases, fine-tuning the model \cite{Martinc2020}. 

An alternative to LSCD that utilizes grammatical profiling instead of contextualized word embeddings has been shown to be just as effective, if not more \cite{grammaticalprofiling}, with the added benefit that semantic change that is detected is significant more traceable than its counterpart that uses contextualized word embeddings. That said, its performance is contingent on understanding which grammatical features are correlated to semantic change. Which features are significant varies from language to language, and thus requires resources such as an existing corpus that is annotated with semantic information.

It should also be noted that many of these related works worked with languages such as German, Swedish, and English, that had access to resources such as SemEval, and therefore had the ability to correlate their findings with human-annotated gold standards. Modern Hebrew does not have this luxury, which puts a significant limitation on what we are able to observe.
\section{Experimental Design}

\subsection{Preprocessing the JEMH}

\subsubsection{Lemmatization}
After tokenization, the corpus is lemmatized in order to relieve any bias created by Hebrew's morphological richness. To do this, we utilize Yet Another Parser (YAP), a morphological analyzer for Modern Hebrew, written in Go \cite{yap}. Word forms, lemmas, POS tags, and any relevant morphological features are collected from YAP's morphological disambiguator. 
\subsubsection{Generating Contextualized Word Embeddings}
To generate contextualized word embeddings, we fed the lemmatized corpus through AlephBERT, a large distribution of BERT, pre-trained on a variety of corpora, including Modern Hebrew Twitter, Wikipedia, and OSCAR \cite{alephBERT}. The raw contextualized word embeddings themselves were generated from its hidden states. We choose to omit any fine-tuning of the BERT model, due to its mixed results.

\subsection{Measuring Distance}
\label{distance}
% need section on word usage matrix representation!!!
To measure diachronic lexical semantic change of a token, we use 4 different methods, per \cite{Kutuzov2020,Giulianelli2020}. Given two word usage matrices, $\textbf{u}^{t}_w, \textbf{u}^{t'}_w$, representing token $w$ in two time periods $t, t'$, respectively, and their prototypes\footnote{The prototype of a word usage matrix $\textbf{u}^t_w$ may be calculated as follows: $\textsc{prototype}(\textbf{u}^t_w) = \frac{\sum_{x_i \in \textbf{u}^t_w} x_i}{N^t_w}$} $p^{t}_w, p^{t'}_w$, respectively:

\subsubsection{Inverse Cosine Over Word Prototypes}
Inverse Cosine Over Word Prototypes (PRT) measures the inverted cosine similarity $d$ between the prototypes ("averages") of two given word matrices $\textbf{u}^{t}_w, \textbf{u}^{t'}_w$:
$$\textsc{PRT}(\textbf{u}^{t}_w, \textbf{u}^{t'}_w) = d(p^{t}_w, p^{t'}_w)^{-1}$$
% $$PRT(\textbf{u}^{t}_w, \textbf{u}^{t'}_w) = d(\frac{\sum_{x_i \in \textbf{u}^{t}_w} x_i}{N^{t}_{w}}, \frac{\sum_{x_i \in \textbf{u}^{t'}_w} x_i}{N^{t'}_{w}})^{-1}$$
Because a prototype is a relatively shallow representation of a word usage matrix, PRT only gives an idea of the absolute diachronic movement of a word usage matrix in space, and does not explicitly embed how the structure of the word usage matrix changes over time.

\subsubsection{Average Pairwise Cosine Distance Between Token Embeddings}
Average Pairwise Cosine Distance Between Token Embeddings (APD) sums all cosine distances between every possible pair of embeddings in each given word usage matrix $\textbf{u}^{t}_w, \textbf{u}^{t'}_w$, which is then divided by the aggregate number of possible embedding pairs:
$$\textsc{APD}(\textbf{u}^{t}_w, \textbf{u}^{t'}_w) = \frac{1}{N^{t}_w \cdot N^{t'}_w} \sum_{x_i \in \textbf{u}^{t}_w, x_j \in \textbf{u}^{t'}_w} d(x_i, x_j)$$
Like PRT, since APD also utilizes a somewhat shallow understanding of each word usage matrix, it also does not embed any meaningful information of how the structure of the word usage matrix changes over time. Thus, it may be interpreted as the average distance each individual embedding moves over time.\\

Because APD has a significant computational load, with a worst-case time complexity of $O(n^2)$, we chose to run it on samples of each word usage matrices in the event that $N^{t}_w \cdot N^{t'}_w > 256$. How large these samples were was calculated by the following formula for each pair of word usage matrices:
$$n(\textbf{u}^{t}_w, \textbf{u}^{t'}_w) = \frac{256}{N^{t}_w \cdot N^{t'}_w}$$
While not ideal, this gave us a reasonable cap on the number of possible computations that could occur that maintained the relative sizes of each word usage matrix. % reword this

\subsubsection{Jensen-Shannon Divergence}
\label{JSD}
Jensen-Shannon Divergence (JSD) measures how the distribution of word-senses for each word usage matrix $\textbf{u}^{t}_w, \textbf{u}^{t'}_w$ change over time, where $H$ is the Boltzmann-Gibbs-Shannon Entropy:
\small $$\textsc{JSD}(\textbf{u}^{t}_w, \textbf{u}^{t'}_w) = H(\frac{1}{2}(\textbf{u}^{t}_w + \textbf{u}^{t'}_w)) - \frac{1}{2}(H(\textbf{u}^{t}_w) - H(\textbf{u}^{t'}_w))$$
The word usage matrices above must be clustered to obtain the distribution of word-senses mentioned above. How we obtained said clusterings is discussed in the \ref{clustering}.
\subsubsection{Difference between Token Embedding Diversities}
Difference between Token Embedding Diversity (DIV) measures the difference in the average distance of a given embedding in a word usage matrix to its prototype between two word usage matrices $\textbf{u}^{t}_w, \textbf{u}^{t'}_w$:
% align with amsmath or reformat
$$\textsc{DIV}(\textbf{u}^{t}_w, \textbf{u}^{t'}_w) = $$ 
$$\Bigg | \frac{\sum_{x_i \in \textbf{u}^{t}_w} d(x_i, p^{t}_w)}{N^{t}_w} - \frac{\sum_{x_j \in \textbf{u}^{t'}_w} d(x_j, p^{t'}_w)}{N^{t'}_w} \Bigg |$$

\subsection{Clustering}
\label{clustering}
As mentioned in \ref{JSD}, in order to calculate the Jensen-Shannon Divergence between two given word usage matrices, they must first be clustered in order to get their word usage distributions. There appears to be several schools of thought on how this should be done:
\begin{itemize}
    \item{Following \cite{Martinc2020,Kutuzov2020}, the clusters could be obtained using Affinity Propagation. Although this has a significant time complexity, it does not require a predefined number of clusters.}
    \item{Following \cite{Laicher2021}, the clusters could be obtained using Agglomerative Clustering, with the number of clusters found by the Silhouette Method.}
    \item{Following \cite{Giulianelli2020}, the clusters could be obtained by using K-Means Clustering, with the number of clusters found by the Silhouette Method. The clusters are then run through Expectation Maximization (EM) for 10 iterations to reduce distortion.}
\end{itemize}

After preprocessing a given word usage matrix pair $\textbf{u}^{t}_w, \textbf{u}^{t'}_w$ by subtracting the mean and normalizing them to unit variance, we chose to cluster the word usage matrices together with Affinity Propagation (AP), following \cite{Martinc2020,Kutuzov2020}. Although the change in word usage is difficult to track due to the fact that AP naturally creates many clusters, it doesn't rely on an arbitrarily-set number of clusters which may not necessarily represent each sense in which a given word is used. To account for computational complexity, we randomly sampled each word usage matrix to a maximum size of 1,024 vectors (the maximum word usage matrix size to be clustered therefore has a maximum of 2,048 vectors).

\section{Observations}
Politically charged words, such as ethnic groups, governing bodies, and wars had particular interesting results. 

\subsection{Example 1: Le'umi (trans. 'national')}
The word "Le'umi" has several significantly different word-senses (not an exhaustive list):
\begin{itemize}
    \item Le'umi: National
    \item Bank Le'umi: Name of a historical bank in Israel
    \item Bituach Le'umi: Israeli health insurance system
\end{itemize}

As shown in in \ref{fig.1} and \ref{fig.2}, the PRT, APD, and DIV values of the word "le'umi" do not change much over time. While the explanation for the PRT feels rather obvious, considering the lack of movement shown in the graphs in \ref{Leumi1} through \ref{Leumi5}, why the DIV and APD values have such minute changes is unclear. That said, the JSD measure was significantly more sensitive; it is easy to track the change in clusters over time. Overall, the word-senses of each time period do appear to be relatively stable.
\begin{figure}[!h]
\begin{center}
%\fbox{\parbox{6cm}{
%This is a figure with a caption.}}
% old picture \includegraphics[scale=0.5]{lrec2020W-image1.eps} 
\includegraphics[scale=0.5]{LREC_PAPER/leumi/לאומי_prt.png}
\caption{PRT over time of the word "Le'umi".}
\label{fig.1}
\end{center}
\end{figure}
\begin{figure}[!h]
\begin{center}
%\fbox{\parbox{6cm}{
%This is a figure with a caption.}}
% old picture \includegraphics[scale=0.5]{lrec2020W-image1.eps} 
\includegraphics[scale=0.5]{LREC_PAPER/leumi/לאומי_div.png}
\caption{DIV over time of the word "Le'umi".}
\label{fig.2}
\end{center}
\end{figure}
\begin{figure}[!h]
\begin{center}
%\fbox{\parbox{6cm}{
%This is a figure with a caption.}}
% old picture \includegraphics[scale=0.5]{lrec2020W-image1.eps} 
\includegraphics[scale=0.5]{LREC_PAPER/leumi/לאומי_jsd.png}
\caption{JSD over time of the word "Le'umi".}
\label{fig.3}
\end{center}
\end{figure}
\begin{figure}[!h]
\begin{center}
%\fbox{\parbox{6cm}{
%This is a figure with a caption.}}
% old picture \includegraphics[scale=0.5]{lrec2020W-image1.eps} 
\includegraphics[scale=0.5]{LREC_PAPER/leumi/לאומי_apd.png}
\caption{APD over time of the word "Le'umi".}
\label{fig.4}
\end{center}
\end{figure}

\subsection{Example 2: Aravi (trans. 'Arab')}
Unlike the word "Le'umi", the word "Aravi" presents a more volatile semantic history, which is most obvious in the DIV and JSD measures. Looking at the figures \ref{Aravi1} through \ref{Aravi4}, it seems that this is caused by the word slowly forming distinct word-senses over time, peaking around 1935. Around that time was the Arab Revolt of 1936-1939, which may have contributed to the significant semantic drift.
\begin{figure}[!h]
\begin{center}
%\fbox{\parbox{6cm}{
%This is a figure with a caption.}}
% old picture \includegraphics[scale=0.5]{lrec2020W-image1.eps} 
\includegraphics[scale=0.5]{LREC_PAPER/aravi/ערבי_prt.png}
\caption{PRT over time of the word "Aravi".}
\label{fig.5}
\end{center}
\end{figure}

\begin{figure}[!h]
\begin{center}
%\fbox{\parbox{6cm}{
%This is a figure with a caption.}}
% old picture \includegraphics[scale=0.5]{lrec2020W-image1.eps} 
\includegraphics[scale=0.5]{LREC_PAPER/aravi/ערבי_div.png}
\caption{DIV over time of the word "Aravi".}
\label{fig.6}
\end{center}
\end{figure}

\begin{figure}[!h]
\begin{center}
%\fbox{\parbox{6cm}{
%This is a figure with a caption.}}
% old picture \includegraphics[scale=0.5]{lrec2020W-image1.eps} 
\includegraphics[scale=0.5]{LREC_PAPER/aravi/ערבי_jsd.png}
\caption{JSD over time of the word "Aravi".}
\label{fig.7}
\end{center}
\end{figure}

\begin{figure}[!h]
\begin{center}
%\fbox{\parbox{6cm}{
%This is a figure with a caption.}}
% old picture \includegraphics[scale=0.5]{lrec2020W-image1.eps} 
\includegraphics[scale=0.5]{LREC_PAPER/aravi/ערבי_apd.png}
\caption{APD over time of the word "Aravi".}
\label{fig.8}
\end{center}
\end{figure}
\section{Conclusion}
\subsection{Future Work}
While this experiment was interesting, again, it is not guaranteed that semantic change detected via contextualized word embeddings can be understood a priori without extensive manual work. Our findings could be juxtaposed with tracking semantic change via grammatical profiling, per \cite{grammaticalprofiling}. Particularly, there is some work on the semantic values of Hebrew verbal templates, known as \textit{binyanim}, that that they are worth investigating with computational methods.
%\section{Citing References in the Text}

%\subsection{Language Resource References}

%\subsubsection{When Citing Language Resources}
%As you may know, LREC introduced a separate section on Language Resources citation to enhance the value of such assets. When citing language resources, we recommend to proceed in the same way as for bibliographical references. Please make sure to compile your Language Resources stored as a .bib file \textbf{separately} (BibTex under pdfLaTeX). This produces the required .aux et .bbl files. Thus, a language resource should be cited as \citelanguageresource{Speecon} and \citelanguageresource{EMILLE} .

%\subsection{Big tables}
%
%An example of a big table which extends beyond the column and will
%float in the next page.
%
% \begin{table*}[ht]
% \begin{center}
% \begin{tabular}{|l|l|}
%
%       \hline
%       Level&Tools\\
%       \hline\hline
%       Morphology & Pitrat Analyser\\
%       Syntax & LFG Analyser (C-Structure)\\
%       Semantics & LFG F-Structures + Sowa's Conceptual Graphs  \\
%       \hline
%
% \end{tabular}
% \caption{The caption of the big table}
% \end{center}
% \end{table*}
%

\section{Acknowledgements}

Place all acknowledgements (including those concerning research grants and funding) in a separate section at the end of the paper.

\nocite{*}
\section{Bibliographical References}\label{reference}
%\label{main:ref}

\bibliographystyle{lrec2022-bib}
\bibliography{lrec2022-example}

\section{Language Resource References}
% need to fix this
\label{lr:ref}
\bibliographystylelanguageresource{lrec2022-bib}
\bibliographylanguageresource{languageresource}

\section*{Appendices: CWE Representations of Tokens}
These plots contain Principal Component Analysis (PCA) representations of contextualized word embeddings of the words "Le'umi" and "Aravi" in a given time period, where each embedding is reduced to 3 components (one is expressed by color). It should be noted that because the embeddings have been run through PCA, they are only approximate representations of the embeddings in their full, 768-dimensional form. The axes therefore don't have any meaning beyond relative distance between each point representing an embedding.

\section*{Appendix: Contextual Word Embedding Graphs of "Le'umi"}
\begin{figure}[!h]
\begin{center}
%\fbox{\parbox{6cm}{
%This is a figure with a caption.}}
% old picture \includegraphics[scale=0.5]{lrec2020W-image1.eps} 
\includegraphics[scale=0.5]{LREC_PAPER/leumi_cwes/1910.png}
\caption{"Le'umi", 1910-1915.}
\label{Leumi1}
\end{center}
\end{figure}

\begin{figure}[!h]
\begin{center}
%\fbox{\parbox{6cm}{
%This is a figure with a caption.}}
% old picture \includegraphics[scale=0.5]{lrec2020W-image1.eps} 
\includegraphics[scale=0.5]{LREC_PAPER/leumi_cwes/1915.png}
\caption{"Le'umi", 1915-1920.}
\label{Leumi2}
\end{center}
\end{figure}

\begin{figure}[!h]
\begin{center}
%\fbox{\parbox{6cm}{
%This is a figure with a caption.}}
% old picture \includegraphics[scale=0.5]{lrec2020W-image1.eps} 
\includegraphics[scale=0.5]{LREC_PAPER/leumi_cwes/1920.png}
\caption{"Le'umi", 1920-1925.}
\label{Leumi3}
\end{center}
\end{figure}

\begin{figure}[!h]
\begin{center}
%\fbox{\parbox{6cm}{
%This is a figure with a caption.}}
% old picture \includegraphics[scale=0.5]{lrec2020W-image1.eps} 
\includegraphics[scale=0.5]{LREC_PAPER/leumi_cwes/1925.png}
\caption{"Le'umi", 1925-1930.}
\label{Leumi4}
\end{center}
\end{figure}

\begin{figure}[!h]
\begin{center}
%\fbox{\parbox{6cm}{
%This is a figure with a caption.}}
% old picture \includegraphics[scale=0.5]{lrec2020W-image1.eps} 
\includegraphics[scale=0.5]{LREC_PAPER/leumi_cwes/1930.png}
\caption{"Le'umi", 1930-1935.}
\label{Leumi5}
\end{center}
\end{figure}

\section*{Appendix: Contextual Word Embedding Graphs of "Aravi"}
\begin{figure}[!h]
\begin{center}
%\fbox{\parbox{6cm}{
%This is a figure with a caption.}}
% old picture \includegraphics[scale=0.5]{lrec2020W-image1.eps} 
\includegraphics[scale=0.5]{LREC_PAPER/aravi_cwes/aravi_1920.png}
\caption{"Aravi", 1920-1925.}
\label{Aravi1}
\end{center}
\end{figure}

\begin{figure}[!h]
\begin{center}
%\fbox{\parbox{6cm}{
%This is a figure with a caption.}}
% old picture \includegraphics[scale=0.5]{lrec2020W-image1.eps} 
\includegraphics[scale=0.5]{LREC_PAPER/aravi_cwes/aravi_1925.png}
\caption{"Aravi", 1925-1930.}
\label{Aravi2}
\end{center}
\end{figure}

\begin{figure}[!h]
\begin{center}
%\fbox{\parbox{6cm}{
%This is a figure with a caption.}}
% old picture \includegraphics[scale=0.5]{lrec2020W-image1.eps} 
\includegraphics[scale=0.5]{LREC_PAPER/aravi_cwes/aravi_1930.png}
\caption{"Aravi", 1930-1935.}
\label{Aravi3}
\end{center}
\end{figure}

\begin{figure}[!h]
\begin{center}
%\fbox{\parbox{6cm}{
%This is a figure with a caption.}}
% old picture \includegraphics[scale=0.5]{lrec2020W-image1.eps} 
\includegraphics[scale=0.5]{LREC_PAPER/aravi_cwes/aravi_1935.png}
\caption{"Aravi", 1935-1940.}
\label{Aravi4}
\end{center}
\end{figure}

\end{document}
